{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1lBWjTH7EVO2"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, DepthwiseConv2D"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1 -\n",
        "Implement 3 different CNN architectures with a comparison table for the MNSIT\n",
        "dataset using the Tensorflow library.\n",
        "Note -\n",
        "1. The model parameters for each architecture should not be more than 8000\n",
        "parameters\n",
        "2. Code comments should be given for proper code understanding.\n",
        "3. The minimum accuracy for each accuracy should be at least 96%"
      ],
      "metadata": {
        "id": "M7GR6oOHEXMW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
      ],
      "metadata": {
        "id": "0qKs31Q4EYkn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce16e8cc-8166-469e-95c9-bdfcc2ebb0d0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KwvRH_gnFd4r",
        "outputId": "b6e505f4-8345-428f-fb36-3b4ef036191d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalise the pixel values to range 0-1 and reshape\n",
        "x_train = x_train.reshape((x_train.shape[0], 28,28,1)).astype('float32') / 255\n",
        "x_test = x_test.reshape((x_test.shape[0],28,28,1)).astype('float32') / 255"
      ],
      "metadata": {
        "id": "luQOv2E2NVzU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#one hot encoding the lables\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)"
      ],
      "metadata": {
        "id": "ClCDDdmVSFHD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load and preprocess the dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Normalize and reshape the data\n",
        "x_train = x_train.reshape((x_train.shape[0], 28, 28, 1)).astype('float32') / 255\n",
        "x_test = x_test.reshape((x_test.shape[0], 28, 28, 1)).astype('float32') / 255\n",
        "\n",
        "# One-hot encode the labels\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# Architecture 1: Small CNN with 6,810 parameters\n",
        "model1 = models.Sequential([\n",
        "    layers.Conv2D(4, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Architecture 2: Depthwise CNN with 6,650  parameters\n",
        "model2 = models.Sequential([\n",
        "    layers.Conv2D(16, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    layers.DepthwiseConv2D((3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(16, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Architecture 3: Strided CNN with 5,930 parameters\n",
        "model3 = models.Sequential([\n",
        "    layers.Conv2D(20, (3, 3), strides=(2,2), activation='relu', input_shape=(28, 28, 1)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "# Function to compile, train, and evaluate models\n",
        "def train_and_evaluate(model, x_train, y_train, x_test, y_test, optimizer='adam', epochs=5, batch_size=64):\n",
        "    print(\"optimzer\", optimizer, \"epochs\", epochs, \"batch_size\", batch_size)\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0)\n",
        "    test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
        "    return test_acc, model.count_params()\n",
        "\n",
        "# Train and evaluate all models\n",
        "results = {}\n",
        "models_list = [model1, model2, model3]\n",
        "model_names = ['Model 1', 'Model 2', 'Model 3']\n",
        "optimizers = ['adam', 'sgd', 'rmsprop']\n",
        "epochs = [15, 10, 7]\n",
        "\n",
        "for model, name, optimizer, epoch in zip(models_list, model_names, optimizers, epochs):\n",
        "    acc, params = train_and_evaluate(model, x_train, y_train, x_test, y_test, optimizer, epoch)\n",
        "    results[name] = {'Accuracy': acc, 'Parameters': params, 'Optimizer':optimizer}\n",
        "\n",
        "# Print results in a comparison table\n",
        "print(f\"{'Model':<10} {'Accuracy':<10} {'Parameters':<10} {'Optimizer':<10}\")\n",
        "for model, metrics in results.items():\n",
        "    print(f\"{model:<10} {metrics['Accuracy']:<10.4f} {metrics['Parameters']:<10} {metrics['Optimizer']:10}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DaKgJOXfW3qa",
        "outputId": "fa146847-6b2b-455e-8c2e-45f622e82e88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "optimzer adam epochs 15 batch_size 64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2 -\n",
        "Implement 5 different CNN architectures with a comparison table for CIFAR 10\n",
        "dataset using the PyTorch library\n",
        "Note -\n",
        "1. The model parameters for each architecture should not be more than 10000\n",
        "parameters\n",
        "2. Code comments should be given for proper code understanding"
      ],
      "metadata": {
        "id": "6oX3ruaGVLPT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torchsummary import summary\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader\n",
        "from time import time"
      ],
      "metadata": {
        "id": "WK2M4OAv1hlh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"using Device {device}\")\n",
        "if torch.cuda.is_available():\n",
        "  print(f\"cuda name is {torch.cuda.get_device_name(0)}\")\n",
        "# Hyperparameters\n",
        "batch_size = 32\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Data preprocessing\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(train_dataset.data.shape[1:])\n",
        "print(train_loader.dataset.data.shape[1:])"
      ],
      "metadata": {
        "id": "WhjoDkeKNGM8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "448f0e7b-c368-4610-cc11-2c9c67a9ad7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "\n",
            "Model 1 Summary:\n",
            "model---> Sequential(\n",
            "  (0): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (1): ReLU()\n",
            "  (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (3): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (4): ReLU()\n",
            "  (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (6): Flatten(start_dim=1, end_dim=-1)\n",
            "  (7): Linear(in_features=1024, out_features=10, bias=True)\n",
            ")\n",
            "Epoch 1, Loss: 1.5099\n",
            "Epoch 2, Loss: 1.2314\n",
            "Epoch 3, Loss: 1.1173\n",
            "Epoch 4, Loss: 1.0482\n",
            "Epoch 5, Loss: 1.0059\n",
            "Accuracy: 62.43%\n",
            "\n",
            "Model 2 Summary:\n",
            "model---> Sequential(\n",
            "  (0): Conv2d(3, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (1): ReLU()\n",
            "  (2): Conv2d(6, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (3): ReLU()\n",
            "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (5): Conv2d(12, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (6): ReLU()\n",
            "  (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (8): Flatten(start_dim=1, end_dim=-1)\n",
            "  (9): Linear(in_features=1536, out_features=10, bias=True)\n",
            ")\n",
            "Epoch 1, Loss: 1.4936\n",
            "Epoch 2, Loss: 1.1406\n",
            "Epoch 3, Loss: 1.0179\n",
            "Epoch 4, Loss: 0.9436\n",
            "Epoch 5, Loss: 0.8971\n",
            "Accuracy: 66.09%\n",
            "\n",
            "Model 3 Summary:\n",
            "model---> Sequential(\n",
            "  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (1): ReLU()\n",
            "  (2): Conv2d(4, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (3): ReLU()\n",
            "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (5): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (6): ReLU()\n",
            "  (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (8): Flatten(start_dim=1, end_dim=-1)\n",
            "  (9): Linear(in_features=1024, out_features=10, bias=True)\n",
            ")\n",
            "Epoch 1, Loss: 1.5938\n",
            "Epoch 2, Loss: 1.2797\n",
            "Epoch 3, Loss: 1.1684\n",
            "Epoch 4, Loss: 1.1017\n",
            "Epoch 5, Loss: 1.0524\n",
            "Accuracy: 61.55%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_trainable_params(model):\n",
        "  return sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ],
      "metadata": {
        "id": "I7g8M3bvv2nL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN1(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3, stride=1, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        nn.Conv2d(in_channels=8, out_channels=13, kernel_size=3, stride=1, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(13 * 8 * 8, 10)\n",
        "    )\n",
        "    self.to(device)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "class CNN2(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1), # (input_width - Filter_size + 2(padding)) / Stripe + 1 = (32 - 3 + 2(0)) / 2 + 1\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2), # = (16-2)/1 +1\n",
        "        nn.Conv2d(in_channels=16, out_channels=31, kernel_size=3, stride=2, padding=1), # (16 - 3)  / 1 +1\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(31 * 4 * 4, 10)\n",
        "    )\n",
        "    self.to(device)\n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "class CNN3(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CNN3, self).__init__()\n",
        "    self.conv1 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=3, out_channels=14, kernel_size=3, stride=2, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(14 * 8 * 8, 10)\n",
        "    )\n",
        "    self.to(device)\n",
        "\n",
        "  def forward(self, x):\n",
        "      x = self.conv1(x)\n",
        "      return x\n"
      ],
      "metadata": {
        "id": "UufguWOu0nJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_evaluate(model, optimizer_name='adam', epochs=5, lr=0.001):\n",
        "  model_name = model.__class__.__name__\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  if optimizer_name.lower() == 'adam':\n",
        "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "  elif optimizer_name == 'sgd':\n",
        "      optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "  elif optimizer_name == 'rms':\n",
        "      optimizer = optim.RMSprop(model.parameters(), lr=lr)\n",
        "  else:\n",
        "      raise ValueError(\"Invalid optimizer name. Choose from 'adam', 'sgd', or 'rms'.\")\n",
        "  start_time = time()\n",
        "  for epoch in range(epochs):\n",
        "      for i, (images, labels) in enumerate(train_loader):\n",
        "          images = images.to(device)\n",
        "          labels = labels.to(device)\n",
        "\n",
        "          # Forward pass\n",
        "          outputs = model(images)\n",
        "          loss = criterion(outputs, labels)\n",
        "\n",
        "          # Backward and optimize\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          if (i + 1) % 1500 == 0:\n",
        "              print(f'{model_name} Epoch [{epoch + 1}/{epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
        "\n",
        "  end_time = time()\n",
        "  print(f\"Model name: {model_name}, Training time: {(end_time - start_time):.2f}s\")\n",
        "\n",
        "  # Evaluation\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "      correct = 0\n",
        "      total = 0\n",
        "      for images, labels in test_loader:\n",
        "          images = images.to(device)\n",
        "          labels = labels.to(device)\n",
        "          outputs = model(images)\n",
        "          _, predicted = torch.max(outputs.data, 1)\n",
        "          total += labels.size(0)\n",
        "          correct += (predicted == labels).sum().item()\n",
        "\n",
        "      accuracy = 100 * correct / total\n",
        "      print(f'Accuracy of the model {model_name} on the test images: {accuracy:.2f}%')\n",
        "      return accuracy, count_trainable_params(model), end_time - start_time\n"
      ],
      "metadata": {
        "id": "Eg2iMIqc0nOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and evaluate all models\n",
        "results = {}\n",
        "models_list = [CNN1(), CNN2(), CNN3()]\n",
        "model_names = ['Model_1','Model_2', 'Model_3']\n",
        "optimizers = ['rms', 'adam', 'sgd']\n",
        "epochs = [10, 5, 5]\n",
        "\n",
        "for model, name, optimizer, epoch in zip(models_list, model_names, optimizers, epochs):\n",
        "    acc, params, training_time = train_and_evaluate(model, optimizer, epoch)\n",
        "    results[name] = {'Accuracy': acc, 'Parameters': params, 'Optimizer':optimizer, 'training_time': training_time}\n",
        "\n",
        "# Print results in a comparison table\n",
        "print(f\"{'Model':<10} {'Accuracy':<10} {'Parameters':<10} {'Optimizer':<10} {'Training time':<10}\")\n",
        "for model, metrics in results.items():\n",
        "    print(f\"{model:<10} {metrics['Accuracy']:<10.3f} {metrics['Parameters']:<10} {metrics['Optimizer']:10} {metrics['training_time']:10.2f}\")"
      ],
      "metadata": {
        "id": "D7MMT0x71CHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uz2V4ylo0nR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "suzRn9Ig0nVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n-oNkjOy0nY-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}