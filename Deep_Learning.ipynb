{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1lBWjTH7EVO2"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, DepthwiseConv2D"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1 -\n",
        "Implement 3 different CNN architectures with a comparison table for the MNSIT\n",
        "dataset using the Tensorflow library.\n",
        "Note -\n",
        "1. The model parameters for each architecture should not be more than 8000\n",
        "parameters\n",
        "2. Code comments should be given for proper code understanding.\n",
        "3. The minimum accuracy for each accuracy should be at least 96%"
      ],
      "metadata": {
        "id": "M7GR6oOHEXMW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
      ],
      "metadata": {
        "id": "0qKs31Q4EYkn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a146fb2-5634-4c5a-efac-669a8cddd42b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KwvRH_gnFd4r",
        "outputId": "13c05d13-3a7d-4d23-e7c1-5204ed34f090"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalise the pixel values to range 0-1 and reshape\n",
        "x_train = x_train.reshape((x_train.shape[0], 28,28,1)).astype('float32') / 255\n",
        "x_test = x_test.reshape((x_test.shape[0],28,28,1)).astype('float32') / 255"
      ],
      "metadata": {
        "id": "luQOv2E2NVzU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#one hot encoding the lables\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)"
      ],
      "metadata": {
        "id": "ClCDDdmVSFHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load and preprocess the dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Normalize and reshape the data\n",
        "x_train = x_train.reshape((x_train.shape[0], 28, 28, 1)).astype('float32') / 255\n",
        "x_test = x_test.reshape((x_test.shape[0], 28, 28, 1)).astype('float32') / 255\n",
        "\n",
        "# One-hot encode the labels\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# Architecture 1: Small CNN with 6,810 parameters\n",
        "model1 = models.Sequential([\n",
        "    layers.Conv2D(4, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Architecture 2: Depthwise CNN with 6,650  parameters\n",
        "model2 = models.Sequential([\n",
        "    layers.Conv2D(16, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    layers.DepthwiseConv2D((3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(16, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Architecture 3: Strided CNN with 5,930 parameters\n",
        "model3 = models.Sequential([\n",
        "    layers.Conv2D(20, (3, 3), strides=(2,2), activation='relu', input_shape=(28, 28, 1)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "# Function to compile, train, and evaluate models\n",
        "def train_and_evaluate(model, x_train, y_train, x_test, y_test, optimizer='adam', epochs=5, batch_size=64):\n",
        "    print(\"optimzer\", optimizer, \"epochs\", epochs, \"batch_size\", batch_size)\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0)\n",
        "    test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
        "    return test_acc, model.count_params()\n",
        "\n",
        "# Train and evaluate all models\n",
        "results = {}\n",
        "models_list = [model1, model2, model3]\n",
        "model_names = ['Model 1', 'Model 2', 'Model 3']\n",
        "optimizers = ['adam', 'sgd', 'rmsprop']\n",
        "epochs = [15, 10, 7]\n",
        "\n",
        "for model, name, optimizer, epoch in zip(models_list, model_names, optimizers, epochs):\n",
        "    acc, params = train_and_evaluate(model, x_train, y_train, x_test, y_test, optimizer, epoch)\n",
        "    results[name] = {'Accuracy': acc, 'Parameters': params, 'Optimizer':optimizer}\n",
        "\n",
        "# Print results in a comparison table\n",
        "print(f\"{'Model':<10} {'Accuracy':<10} {'Parameters':<10} {'Optimizer':<10}\")\n",
        "for model, metrics in results.items():\n",
        "    print(f\"{model:<10} {metrics['Accuracy']:<10.4f} {metrics['Parameters']:<10} {metrics['Optimizer']:10}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DaKgJOXfW3qa",
        "outputId": "bfd4713b-de10-45cc-d209-b64c46b35e6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "optimzer adam epochs 15 batch_size 64\n",
            "optimzer sgd epochs 10 batch_size 64\n",
            "optimzer rmsprop epochs 7 batch_size 64\n",
            "Model      Accuracy   Parameters Optimizer \n",
            "Model 1    0.9760     6810       adam      \n",
            "Model 2    0.9748     6650       sgd       \n",
            "Model 3    0.9718     7410       rmsprop   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2 -\n",
        "Implement 5 different CNN architectures with a comparison table for CIFAR 10\n",
        "dataset using the PyTorch library\n",
        "Note -\n",
        "1. The model parameters for each architecture should not be more than 10000\n",
        "parameters\n",
        "2. Code comments should be given for proper code understanding"
      ],
      "metadata": {
        "id": "6oX3ruaGVLPT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchsummary import summary\n",
        "\n",
        "# Define transformations for the CIFAR-10 dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Define the CNN architectures\n",
        "\n",
        "# Model 1: Simple CNN\n",
        "def model_1():\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(3, 8, kernel_size=3, stride=1, padding=1),  # 8 filters\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),                # 16x16x8\n",
        "        nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1), # 16 filters\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),                # 8x8x16\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(16 * 8 * 8, 10)                             # Fully connected layer\n",
        "    )\n",
        "\n",
        "# Model 2: Deeper CNN\n",
        "def model_2():\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(3, 6, kernel_size=3, stride=1, padding=1),  # 6 filters\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(6, 12, kernel_size=3, stride=1, padding=1), # 12 filters\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),                # 16x16x12\n",
        "        nn.Conv2d(12, 24, kernel_size=3, stride=1, padding=1),# 24 filters\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),                # 8x8x24\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(24 * 8 * 8, 10)                             # Fully connected layer\n",
        "    )\n",
        "\n",
        "# Model 3: Compact CNN\n",
        "def model_3():\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(3, 4, kernel_size=3, stride=1, padding=1),  # 4 filters\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(4, 8, kernel_size=3, stride=1, padding=1),  # 8 filters\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),                # 16x16x8\n",
        "        nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1), # 16 filters\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),                # 8x8x16\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(16 * 8 * 8, 10)                             # Fully connected layer\n",
        "    )\n",
        "\n",
        "# Training and evaluation function\n",
        "def train_and_evaluate(model, trainloader, testloader, epochs=5):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in trainloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "        print(f\"Epoch {epoch + 1}, Loss: {running_loss / len(trainloader):.4f}\")\n",
        "\n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in testloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f\"Accuracy: {100 * correct / total:.2f}%\")\n",
        "\n",
        "# Instantiate and summarize models\n",
        "for i, model_fn in enumerate([model_1], 1):\n",
        "    print(f\"\\nModel {i} Summary:\")\n",
        "    model = model_fn()\n",
        "    print(\"model--->\", model)\n",
        "    # summary(model, (3, 32, 32))\n",
        "    train_and_evaluate(model, trainloader, testloader)\n",
        "\n",
        "# Comparison Table:\n",
        "# | Model   | Parameters | Test Accuracy |\n",
        "# |---------|------------|---------------|\n",
        "# | Model 1 | ~7,000     | TBD           |\n",
        "# | Model 2 | ~9,000     | TBD           |\n",
        "# | Model 3 | ~8,500     | TBD           |\n"
      ],
      "metadata": {
        "id": "WhjoDkeKNGM8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "448f0e7b-c368-4610-cc11-2c9c67a9ad7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "\n",
            "Model 1 Summary:\n",
            "model---> Sequential(\n",
            "  (0): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (1): ReLU()\n",
            "  (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (3): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (4): ReLU()\n",
            "  (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (6): Flatten(start_dim=1, end_dim=-1)\n",
            "  (7): Linear(in_features=1024, out_features=10, bias=True)\n",
            ")\n",
            "Epoch 1, Loss: 1.5099\n",
            "Epoch 2, Loss: 1.2314\n",
            "Epoch 3, Loss: 1.1173\n",
            "Epoch 4, Loss: 1.0482\n",
            "Epoch 5, Loss: 1.0059\n",
            "Accuracy: 62.43%\n",
            "\n",
            "Model 2 Summary:\n",
            "model---> Sequential(\n",
            "  (0): Conv2d(3, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (1): ReLU()\n",
            "  (2): Conv2d(6, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (3): ReLU()\n",
            "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (5): Conv2d(12, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (6): ReLU()\n",
            "  (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (8): Flatten(start_dim=1, end_dim=-1)\n",
            "  (9): Linear(in_features=1536, out_features=10, bias=True)\n",
            ")\n",
            "Epoch 1, Loss: 1.4936\n",
            "Epoch 2, Loss: 1.1406\n",
            "Epoch 3, Loss: 1.0179\n",
            "Epoch 4, Loss: 0.9436\n",
            "Epoch 5, Loss: 0.8971\n",
            "Accuracy: 66.09%\n",
            "\n",
            "Model 3 Summary:\n",
            "model---> Sequential(\n",
            "  (0): Conv2d(3, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (1): ReLU()\n",
            "  (2): Conv2d(4, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (3): ReLU()\n",
            "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (5): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (6): ReLU()\n",
            "  (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (8): Flatten(start_dim=1, end_dim=-1)\n",
            "  (9): Linear(in_features=1024, out_features=10, bias=True)\n",
            ")\n",
            "Epoch 1, Loss: 1.5938\n",
            "Epoch 2, Loss: 1.2797\n",
            "Epoch 3, Loss: 1.1684\n",
            "Epoch 4, Loss: 1.1017\n",
            "Epoch 5, Loss: 1.0524\n",
            "Accuracy: 61.55%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I7g8M3bvv2nL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}